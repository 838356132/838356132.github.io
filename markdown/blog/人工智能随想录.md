
#!title:    人工智能随想录
#!date:     2018-12-09
#!authors:  Mikukonai
#!cover:    ./image/cover/70385021_p1.jpg
#!type:     原创
#!tags:     

#!content

> : **A**rtificial **I**diot
人 工 智 障

# AI基础

## 形式化方法和机器学习的关系

> 问题来自知乎，下文是对此问题的回答。回答时间2019.2.18。

作为一线社畜的一点理解：

确定性和随机性，是同一个问题——也就是复杂性问题——的两个方面。

机器学习：

- 思想核心：从数据出发，自底向上地拟合现实世界。
- 优势：大规模模型可以精确地处理复杂模式，好的模型具有良好的泛化能力。
- 劣势：几乎没有可解释性，微观层面上近乎玄学；需要投喂海量训练数据，对机器要求高。

形式化方法：

- 思想核心：从逻辑出发，自顶向下地拟合现实世界。
- 优势：精确、可信、结构化、可解释、可预测，能处理高阶的、抽象的模式（概念和类型）。
- 劣势：难以处理非确定性的问题；需要领域专家和逻辑专家介入，对人的要求高，对机器的要求也不低（划掉）。

NLP（自然语言处理）是人工智能皇冠上的明珠。以NLP领域为例，在机器学习/深度学习方向，诸如Transformer、BERT之类的深度神经网络模型，在诸多任务中甚至超过了人类的水平。当然，如此优秀的性能，背后是海量的数据和算力投入。最近（2019.2）的SOTA：GPT-2，号称拥有15亿参数，使用了40GB训练数据和256个TPU。可见，统计这条路走到现在，已经到了大力出奇迹的阶段，这也就意味着马太效应会越来越明显。

但是，尽管机器学习/深度学习在模式识别和预测的问题上取得了非常好的性能，但是它目前仍然有一个重大的问题，那就是模型尚不能理解高层次的、抽象的、全局的或者长距离的特征之间的关联，也就是抽象思维的能力。有人开玩笑说，GPT-2再牛逼，也不可能帮曹雪芹续写《红楼梦》后40章吧。在中学的语文课上，我们讲到散文的时候，经常会说到一个词叫“形散神不散”。如何使得机器具备形神兼备的思维能力，就要依靠形式化方法，给机器以先验的逻辑能力了。

形式化方法在NLP领域应用比较多的方向，当属语义网和知识图谱。语义网与其说是一门技术、一门标准、不如说是一门愿景。但是现在看来这个愿景已经越来越不清晰了。知识图谱是语义网的延伸，它的初衷是将泛在的、非结构化的数据中蕴含的结构和语义关联，表达成显式的图状结构。结构一旦建立，即可利用现有的数理逻辑工具，例如Horn子句、一阶逻辑、自动机等等，验证、挖掘并预测知识图谱中存在的或还没有的知识。

一旦机器具备了形式化、结构化思考的能力，与神经网络模型赋予它强大的模式处理能力结合起来，就很有可能实现“形神兼备”的真·人工智能。当然，这究竟是不是通往强人工智能的不二法门，还需要学术界和产业界共同努力探cai索keng。

正如最开始的比较中所说，形式化方法对人的要求是极高的，知识图谱同样面临这个问题。知识图谱（或者说知识库）是一项系统工程，几乎集齐了NLP领域的大多数问题，那么既然是系统工程，就必然涉及到项目管理的问题。其中，领域本体的设计和确定就是一个核心问题，也是各方博si弈bi的焦点所在。在知识库的全生命周期中，项目管理、领域专家、逻辑学家、架构师、系统工程师、算法工程师、运维工程师、客户都各有各的诉求，各有各的观点，如何在高层次上把握这些问题，其复杂性是远远超出技术本身的。所以文因互联的鲍捷博士有一个观点：项目的结构决定了项目团队的结构，项目的演化决定了项目团队的演化。在知识图谱中，形式化方法主要集中在本体设计上。关于本体，鲍捷博士认为“本体的设计是政治”，就是这个意思。进一步说，数理逻辑的类型论和范畴论，在实际的工程项目中，体现为类型系统的设计，也就是系统架构的设计。架构设计从来不是一个技术问题，而是管理问题，甚至是政治问题；对类型的理解，体现的是项目管理者对于项目的理解，而不是基层工程人员的理解。因此，形式化方法的应用，其实并不是一个简单的技术问题，而是涉及到工程项目全局的复杂的管理问题。

扯了这么多，好像有点偏题了。简单来说，基于统计的方法尽管难以解释，但是只要你投喂足够多足够好的数据，就会得到一个足够满意的模型。但基于形式化方法的方法尽管很靠谱，但它实际上是把许多技术上的问题转嫁到管理或者项目实施的其他环节上了。比较来看，谁也不比谁的代价小，但总的来看，还是机器学习显得更下里巴人一点。毕竟，没有什么问题，不能用一句`import tensorflow as tf`解决的（滑稽

工程师是驾驭复杂性的艺术家。人工智能便是典型的复杂问题，复杂到连“正确”的标准都没有，所以，又谈何“验证”呢。此外，正如最开始所说，从更高层次的世界观来看，确定性和非确定性，二者是同一个问题的不同方面。如果你读过薛定谔的《生命是什么》，对这个问题就会有充分的了解。

从更加功利的角度看，80%的业务只需要解决80%的问题就好了，因为它们活不到需要解决20%的那部分问题的那一天。但是从终极追求的角度看，形式化方法研究的是抽象结构，相比于启发式的人工神经网络研究，逻辑学更加闪耀着人作为“人”的荣耀之光。作为从复杂的物理结构中演化出的高层次知识，以逻辑学为代表的认知智能，是我们人类的终极财富。

毕竟，我们是能够思想的苇草，我们的全部价值就在于思想，我们并不甘心做复读机。


# AI产品化

## 能抓到耗子就是好猫

> 2018.11.1，朋友圈

前段时间我爸妈来南京住了一段时间，跟他们说这个小爱同学厉害的很，他们觉得一个音箱而已这么贵还没什么用。

后来每天都“小爱同学”“小爱同学”地叫她，扫地机器人自然不必说，就连晚上睡觉关灯都懒得用遥控器了。真香。

算下来，我在小米生态链上投入了大概不到一个月的工资，但是对生活质量的提升是成倍的。

我不想将这称为“人工智能”，这与真正的“智能”关系并不大。不过从另一个角度看，仅仅是简单的控制论“智能”（**其实就是家居自动化**），就足以颠覆式地改变人们的生活习惯，这就足以说明许多问题了。

## 零散的想法

- 学会训练用户，不能完全被用户牵着鼻子走：
-- 管理客户预期（由差变好易，由好变差难）
-- 引导客户行为（但是也不要有太强的侵入感）
-- 回应用户关切（交互要有反馈）
-- 跟踪用户重点（有取有舍，有所为有所不为）
- 相应地，不要制造人与机器的对立，而应是合作关系。
- AI是人类的代具，不是谁取代谁的对立关系，而是相互依存共同进化的关系。

- 重视（广义的）交互设计。
- 重视面向人类的那一部分，即人机接口。

- 广义的二八原则和长尾现象：是否要追求完备？
-- 区别对待头部和尾部，数据、用户、场景，等等；
-- 两点论和重点论。

- 图灵测试是否合理？标准并不清晰。不是一个良定义的、量化的标准。
- 关于问答系统如何评价，学界和产业界都不太清楚。然而这很可能是一个教育学或者心理学问题。

- 技术不是大头，但技术是核心。

- 关于冷启动问题：
-- 于开发者，不要排斥规则和简单技巧，先让业务跑起来，再谈迭代演进。不能一口吃个胖子。
-- 于用户……？

- 训练一个人类幼崽到初中毕业的水平尚且要十五年，人工智能的迭代也不可以心急。



# NLP

## 语言模型应当是开放系统

> 首次发表于朋友圈，时间2019年3月3日，缘起是“微信翻译将一些当红艺人的英文名翻译成奇怪的东西”的新闻。

语言的演化是非常快的，但庆幸的是语言底层的演化要远远慢于表面。

刚才看晋语（汾阳话）教程时，发现汾阳话同样存在地名白读稳定的现象。白读反映语言内核，是可以传承的稳定的内核。但是作为一种世界观的再现，语言的演化和这个世界的变化是同步的。

好的语言模型可以学习到语言的底层内核，但是目前来看，它很难增量式地、有选择地学习到语言表面不断变化的那部分。这是一个很深刻的问题——因为人类**知道**哪些是内核，哪些是新概念、新说法、新流行语。

语言模型不仅仅是关于语言的模型，更是关于这个世界的一整套观念。目前训练出的语言模型本质上都是复读机，它们并没有像人类这样的，站在更高层次上理解语言、观察世界的能力。

最后我还是要复读一遍维特根斯坦的话：世界的意义，必定在世界之外。所以，语言模型无法处理的问题，就请交给规则去处理吧。

**神经网络适合从海量语料中挖掘语言的底层模式，而规则和逻辑更适合增量式地、后验地完善和修正现有的语言模型。**

补充一句：一个人能看懂网络上流行的梗、段子、亚文化，背后的代价一定是每天花几个小时刷微博或者抖音这类东西。语言模型为了能够覆盖到这些边边角角的新东西，就必须与人类一起学习，与社会一同进步，与世界同频共振。所以，从唯物辩证法的角度来看，不可能有一旦训练好就一劳永逸的语言模型，只有根据需求和成本考虑做出的取舍而已。

因此，**语言模型的最终形态必然是自适应、自学习、自组织、互联互通的开放系统。**

## 处理NLP问题时要考虑到目标语言的特性

具体语言的性质和模型的架构要综合考虑。例如日语等黏着语，更依赖词缀而不是语序来表达不同的语义，而汉语和英语则正好相反。屈折语更是在词的粒度上就携带了大量语法信息。所以，模型架构的设计，要具体语言具体分析。当然，由于汉语和英语在语法上的相似性（英语正在朝着简化、分析化的方向发展），产业应用时可能未必需要考虑那么多、那么细。

## 关于背景知识和言外之意

- 还有人说投资4.8万就可以移民台湾，天底下哪有这样的好事！
- 美国才是这个时代最大的恶棍。
- 多党轮流执政照样腐败。

## 零散的想法

- 数据存储→数据挖掘→知识工程→知识服务

- 所谓的语义理解可认为是广义的编译过程。自然语言→语义
- “语义”是个非常玄学的东西。现在所知的语义表示有：
-- 自然语言表示（隐式）
-- 形式语言表示（显式、结构化的）
--- 字符串模式
--- 一阶逻辑
--- λ演算（形式语义）
--- 结构化语言（槽）
--- 树、图模型
--- 类别（指称语义？，与分布式表示紧密相关）
-- 分布式表示（隐式）
--- 神经网络
--- 向量空间嵌入
--- 语言模型（概率图模型等）
- 上面这些表示可以互相转化。
- 语义学是一个学科，有许多研究角度。
- 语义和“知识”有什么关系？
- 语义里面还有情感等主观因素。
- 情感是非常微妙的，可以认为是高阶的语义。从AI落地的角度来说，如果脱离应用场景，很难对情感建模。或者，其实也没必要，简单分类足以解决大部分问题。
- 语义必须放在上下文的“闭包”中才有可以解释的意思。
- 例如：中国人的“意思”。

![没别的意思（图片来源见水印）](./image/misc/yisi.jpg)

- 6W2H：what、when、where、who、why、which、how、howmany/much。
- 实现时，首先需要约定“语义”的表示，这是NLU接口的一部分。
- 很难搞清楚NLU这个编译器的输入输出。
- 其实人类也在探索自己是如何思考的。
- 人脑内的知识是分布式表示的。从分布式表示→逻辑是如何跨越的？
- 研究AI就是研究人类自己。

# CV

# 语音

# 参考资料

+ 题图来源：pixiv#70385021


