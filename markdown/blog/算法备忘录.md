
#!title:    算法备忘录
#!date:     2019-03-16
#!authors:  Mikukonai
#!cover:    
#!type:     原创
#!tags:     

#!content

# Levenshtein编辑距离

2018-08-15

## 演示

> <button id="auto" class="MikumarkButton">自动</button> <button id="step" class="MikumarkButton">步进</button> <br>

> <input id="input1" value="sitting">
<input id="input2" value="kitten">

> <div class="edit_distance_line" id="line2"></div><div class="edit_distance_line" id="line1"></div>

> <div id="matrix"></div>

## 原理

[维基百科](https://en.wikipedia.org/wiki/Levenshtein_distance)

递推公式：

![式子中$1_{(a_i \neq b_j)}$意思是：若不等则为1，反之为0。](https://wikimedia.org/api/rest_v1/media/math/render/svg/4520f5376b54613a5b0e6c6db46083989f901821)

------

# 读取WikiData

2018-07-31

为了做命名实体识别，首先考虑使用Wikidata作为初始词库。

参阅[Wikidata:Data_access](https://www.wikidata.org/wiki/Wikidata:Data_access)

Wikidata是开放的、通用的结构化知识库。不仅提供URI、SPARQL等多种分实体查询的接口，甚至还提供JSON、XML、RDF等多种格式的全量转储文件，供所有人自由下载。2018年7月30日的转储数据（压缩后的JSON）有36GB之多，考虑到存储空间限制和一些现实因素，暂且不打算处理这个大文件，而是采用HTTPS直接请求JSON，只保留label和alias，保存成比较小的词库。全量转储仅作为备份。

实体的“名称”称为“label”。一般情况下，我需要的当然是中文的label，但是有的时候并没有中文的，所以读取的时候按照中文中国（zh-cn）、中文（zh）、英文（en）的顺序进行读取。个别情况下，某些实体的标签是空的，此种情况下读出的就是空行。

技术上选择Node.js。其实Python貌似更适合干这种事情，但是我不喜欢Python，所以选择Node.js。

Node很让人不爽的一点（或者说是它的特色或者优势）就是它的异步，所以相比于普通方法，用Node写涉及顺序的东西就很麻烦。此外，通过测试，当请求频率过高的时候，服务器会拒绝请求（超时），所以在下面的代码中，每批向队列中加入300个请求，请求间大概延时100ms，15秒后再处理下一批。经测试，按照这个参数，在家里的网络环境下，可以把失败率压制在可以接受的低水平（每100批20次左右失败）。每轮读取（30000个实体）耗时半小时左右。

代码比较简单，写在这里备忘：

```:javascript
var https = require('https');
var fs = require("fs");
var counter = 0;

// 以下两个参数可以动
const batchIndexFrom = 100;  // 从第x批开始
const batchNumber = 100;     // 读取x批

// 以下参数不要动，因涉及请求频率调整。
const batchSize = 300;
const requestDelay = 100;
const batchDelay = 15000;

function requestHTML(QNumber) {
    let path = `/wiki/Special:EntityData/Q${QNumber}.json`;
    let options = {
        hostname: 'www.wikidata.org',
        port: 443,
        path: path,
        method: 'GET'
    };
    let json = '';
    let req = https.request(options, function (res) {
        res.setEncoding('utf8');
        res.on('data', function (chunk) {
            json += chunk;
        }).on('end', function () {
            if(res.statusCode !== 200) {
                console.log(`Q${QNumber}: <none>`);
            }
            else {
                let result = JSON.parse(json);

                let entityString = `Q${QNumber}:`;

                if(result === undefined || result.entities === undefined || result.entities['Q'+QNumber] === undefined) {
                    return;
                }

                let labels = result.entities['Q'+QNumber].labels;
                let aliases = result.entities['Q'+QNumber].aliases;

                // 标签
                if(labels['zh-cn'] !== undefined) {
                    entityString += labels['zh-cn'].value;
                }
                else if(labels['zh'] !== undefined) {
                    entityString += labels['zh'].value;
                }
                else if(labels['en'] !== undefined) {
                    entityString += labels['en'].value;
                }
                else {}

                // 别名
                if(aliases['zh-cn'] !== undefined) {
                    for(let i = 0; i < aliases['zh-cn'].length; i++) {
                        entityString += ('|' + aliases['zh-cn'][i].value);
                    }
                }
                else if(aliases['zh'] !== undefined) {
                    for(let i = 0; i < aliases['zh'].length; i++) {
                        entityString += ('|' + aliases['zh'][i].value);
                    }
                }
                else if(aliases['en'] !== undefined) {
                    for(let i = 0; i < aliases['en'].length; i++) {
                        entityString += ('|' + aliases['en'][i].value);
                    }
                }
                else {}

                fs.writeFile(`output-${batchIndexFrom}.txt`, entityString + '\n', {flag:'a'}, function(err) {
                    if (err) {
                        return console.error(err);
                    }
                    console.log(`Q${QNumber}写入成功(${counter})`);
                });

                counter++;
            }
        });
    });

    req.on('error', function (e) {
        console.error(e);
        fs.writeFile(`error-${batchIndexFrom}.log`, `Q${QNumber}: ` + e.toString() + '\n', {flag:'a'}, function(err) {
            if (err) {
                return console.error(err);
            }
        });
    });

    req.end();
}

function requestBatch(batchIndex) {
    for(let i = batchSize * batchIndex; i < batchSize * (batchIndex + 1); i++) {
        setTimeout(function() {
            requestHTML(i);
        }, requestDelay * (i - batchSize * batchIndex));
    }
}

for(let i = batchIndexFrom; i < batchIndexFrom + batchNumber; i++) {
    setTimeout(function() {
        console.log(`\n\n\n\n\n\n\n\n==========\n开始第${i}批（本轮读取还剩${batchIndexFrom + batchNumber - i}批）\n==========\n\n\n\n\n\n\n\n`);
        requestBatch(i);
    }, batchDelay * (i - batchIndexFrom));
}
```

读取出来的个别条目是这样的：

```
Q2004:2008年|MMVIII
Q2003:紐芬蘭-拉布拉多|新發地與拉布拉多|新发地与拉布拉多
Q2159:UTC−10:30|UTC-10:30|西10:30區
Q2001:斯坦利·库布里克
Q2005:JavaScript|爪哇脚本|爪本
Q2008:里昂迪奧公園車站
Q2158:巴伦西亚城|Valencia City
Q2157:1919年
Q2011:Gare de Lyon-Vaise
Q2156:1月3日|一月三日
Q2160:沃洛格達河
Q2010:尾田荣一郎|Eiichirō Oda|Tsuki himizu kikondō
Q2007:西北地区|NWT
Q2012:玛雅历
Q2009:育空|Yukon Territory|the Yukon|YT
Q2162:1921年
Q2163:UTC−10:00|UTC-10|西十區|西10區
```

第一个竖线前的是label，后面诸项是aliases。结构比较简单，便于解析，也照顾到人类阅读。

------

# 基于大规模语料统计的新词发现

2017-10-16

> 本文是对顾森（Matrix67）文章 [http://www.csdn.net/article/2013-05-08/2815186]() 和 [http://www.matrix67.com/blog/archives/5044]() 的学习笔记

传统的新词发现方法有一个逻辑上的怪圈：新词发现依赖于分词结果，分词结果依赖于词库，然而新词并不存在于词库中，这样得到的“新词”结果就无法令人信任。为了跳出这个逻辑怪圈，基于大规模语料的新词发现方法抛弃了词库，对大规模语料中可能成词的片段进行分析，得到所有可能的分词结果，再与已有词库进行比对，得到新词。

## 新词特征指标

词是参与成句的最小完整单位。因此，判断一个短片段是否成词，需要从“最小”和“完整”两个方面入手。完整性显得更重要，因此首先考虑短片段组合的稳固程度。

## 内部凝固度

词是更细粒度的词稳定组合在一起形成的，因此使用**凝固度**这一指标衡量若干片段的组合出现的概率大小。

- 对于可能成词AB的片段A和B，两者的出现频率$P(A)$和$P(B)$。
- 若二者并不成词，也即在文本中完全随机出现，则词AB在文本中出现的概率应为$P(A)\cdot P(B)$。
- 而文本中AB的实际出现频率为$P(AB)$：若$P(AB)$远高于$P(A)\cdot P(B)$，那么可初步认为AB是一个成词组合。

按照这个标准，很容易找到诸如“忐忑”、“蜘蛛”、“蟑螂”、“彷徨”这样的双字**联绵词**，因为联绵词中单字单独出现的概率极低。

对于多字词而言，有多种划分片段的方式。划分方式不同，按照上述方法计算得到的$P(A)\cdot P(B)$就截然不同。

> 作为一个无知识库的抽词程序，我们并不知道“电影院”是“电影”加“院”得来的……如果我们把“电影院”看作是“电”加“影院”所得，由此得到的凝合程度会更高一些。

因此，并不能简单地用某一种划分计算得到的$P(A)\cdot P(B)$作为新词内部凝固度指标，而应枚举各种划分方式，选出**最小**的$P(A)\cdot P(B)$，计算$P(AB)$与$P(A)\cdot P(B)$的比值，作为可用的凝固度指标。凝固度指标实际上就是互信息。

> 在整个2400万字的数据中，“电影”一共出现了2774次，出现的概率约为0.000113。“院”字则出现了4797次，出现的概率约为0.0001969。如果两者之间真的毫无关系，它们恰好拼在了一起的概率就应该是0.000113×0.0001969=2.223E–8。但事实上，“电影院”在语料中一共出现了175次，出现概率约为7.183E–6，是预测值的300多倍。

> 类似地，统计可得“的”字的出现概率约为0.0166，因而“的”和“电影”随机组合到了一起的理论概率值为0.0166×0.000113=1.875E–6，这与“的电影”出现的真实概率很接近——真实概率约为1.6E–5，是预测值的8.5倍。

## 运用自由度

内部凝合并不是判断文本片段是否成词的充分条件。如果只考虑内部凝合度的话，并不能确定新词的外部边界，得到的新词很可能会“偏短”。例如：

> 考虑“被子”和“辈子”这两个片段。我们可以说“买被子”、“盖被子”、 “进被子”、“好被子”、“这被子”等，在“被子”前面加各种字；但“辈子”的用法却非常固定，除了“一辈子”、“这辈子”、“上辈子”、“下辈子”，基 本上“辈子”前面不能加别的字了。“辈子”这个文本片段左边可以出现的字太有限，以至于直觉上我们可能会认为，“辈子”并不单独成词，真正成词的其实是 “一辈子”、“这辈子”之类的整体。

因此，判断一个片段是否为新词，除了其内部的具有稳定的结构之外，它本身要足够“自由”去参与成句。换句话说就是，片段的左侧和右侧出现的词汇要有足够大的多样性。

描述“多样性”的指标就是**熵**。对于一个随机系统而言，它的每个状态都蕴含了一定的信息量，状态发生的概率越低，状态提供的信息量就越大。对系统中所有状态具备的信息量按发生概率进行加权平均，就得到了系统的信息熵。也就是说，信息熵是系统信息量的期望。信息熵越高，意味着系统内部的状态越丰富、系统越混乱；信息熵越低，意味着系统内部的状态越贫乏、系统越规律。

新词运用自由度的计算方法如下：

+ 提取左邻字和右邻字集合；
+ 计算每个左右邻字的出现频率并计算左右邻字信息熵；
+ 取二者较小者作为运用自由度指标。

之所以要取二者较小者作为自由度指标，是因为有些词具有固定前（后）缀的特性，例如“共产”（共产党、共产主义…）、“清华”（清华大学、清华学子…）、“夫斯基”这样的后缀词，而这样的词显然是不成词的。

> 在实际运用中你会发现，文本片段的凝固程度和自由程度，两种判断标准缺一不可。只看凝固程度的话，程序会找出“巧克”、“俄罗”、“颜六色”、“柴可夫”等实际上是“半个词”的片段；只看自由程度的话，程序则会把“吃了一顿”、“看了一遍”、 “睡了一晚”、“去了一趟”中的“了一”提取出来，因为它的左右邻字都太丰富了。

------

# Nagao串频统计算法

2017-10-16

<button id="nagao" class="MikumarkButton" style="width:100%;">串频统计</button>

## 引言

词是全文数据中的短小字符序列。下述算法是京都大学的Makoto Nagao于1994年在参考文献[1]中提出的，该算法可以统计全文数据中所有可能的短串及其出现频率。

## 算法说明

1.将文本全文（保留标点、换行等分隔符）读入线性表，形成字符串`S[1:Len]`，字符串长度为`Len`。

```
index  1 2 3 4 5 6 7
    S  庭院深深深几许
```

2.构造指针表`P[1:Len]`：`P[i]`的内容是`i`，代表`S`的后缀子串`S[i:Len]`。

```
index  1 2 3 4 5 6 7
    S  庭院深深深几许

 P[1]  庭院深深深几许
 P[2]    院深深深几许
 P[3]      深深深几许
 P[4]        深深几许
 P[5]          深几许
 P[6]            几许
 P[7]              许
```

3.对所有后缀子串按字典序排序，得到排序后的指针表`PO[j]`。排序采用快速排序，时间复杂度可控制在$O({Len} \log ({Len}))$。

```
     index  1 2 3 4 5 6 7
         S  庭院深深深几许

PO[1] P[6]  几许
PO[2] P[5]  深几许
PO[3] P[4]  深深几许
PO[4] P[3]  深深深几许
PO[5] P[1]  庭院深深深几许
PO[6] P[7]  许
PO[7] P[2]  院深深深几许
```

4.构造“公共前缀串长”表`C[1:Len]`：`C[i]`的内容是`PO[i]`与`PO[i-1]`的①最长②相同③前缀串④的**长度**。特别地，`P[1]`取0。

```
        index  1 2 3 4 5 6 7
            S  庭院深深深几许

C[1] 0  PO[1]  几许
C[1] 0  PO[2]  深几许
C[1] 1  PO[3]  深深几许
C[1] 2  PO[4]  深深深几许
C[1] 0  PO[5]  庭院深深深几许
C[1] 0  PO[6]  许
C[1] 0  PO[7]  院深深深几许
```

5.开始进行串频统计。操作步骤如下：

```
数据结构：S、PO、C，以及存放串频的结果Map
输入：候选词长度N
输出：串频结果Map
1. 取PO[1]的N前缀PN[1]，将<PN[1],1>放入Map；
2. 取PO[i]的N前缀PN[i]：
     若C[i]≥N，则将<PN[i],++>放入Map；（++是自增1函数）
     若C[i]<N，则将<PN[i],1>放入Map；
3. 反复执行2，直到遍历完PO表。
4. 此时得到的Map就是词频统计结果。
```

输入不同的N值，多次执行此步骤，即可得到对“庭院深深深几许”的词频统计结果：

```
庭院   1
院深   1
深深   2
深几   1
几许   1
许     1
庭院深 1
院深深 1
……
```

## 参考文献

+ Nagao M, Mori S. A New Method of N-gram Statistics for Large Number of n and Automatic Extraction of Words and Phrases from Large Text Data of Japanese[J]. Proceedings of Coling, 1994, 1:611-615.
+ 作者不详. Nagao的串频统计算法\[Z/OL\]. [https://wenku.baidu.com/view/9545a9d24431b90d6d85c727.html](), 2015.

------

# 极大团算法

2018-11-23

## 引言

团（clique）指的是无向图的完全子图。其中，如果某个团不是任何其他团的子图，则为**极大团**。顶点数最多的团，称为**最大团**。

求取最大团/极大团的算法往往需要耗费指数级时间，目前尚未找到多项式时间的算法，因而是NP完全问题。使用启发式算法，可以减少一部分时间复杂度，但是没有本质提升。

团算法在许多领域都有用处。例如，朋友圈实际上就是团。在生物信息学中，需要利用团算法解决重复序列的问题。前段时间在工作中遇到了类似的问题，因此在这里写个简单的Demo，记录一下。

## 举例演示

现有一段核酸序列，其上有限制酶A的切割位点3个。使用A处理后，可得到10种长短不一的片段（如下图）。问可以得到的互不重叠的片段组合有哪些？

![ ](./image/misc/dna-cut.png)

从0~9给片段编号，显然，[0,1,2,3]是可能的一种方案，因为它们**两两都没有重叠**。因此，可以将片段间的“不重叠”关系构造成图，求出这个图的所有极大团，就是所有可能的互不重叠的片段组合了。（注：“不重叠”关系是反自反、对称、反传递的关系，因而体现为无向图。）

使用JavaScript实现，计算得到极大团为：<span id="maximal_cliques"></span>

## 极大团算法

最简单的方法当然是暴力地穷举出所有子图，然后逐一检查，这一方案的时间复杂度是指数级的。但是，有很多“显然”的情况是不需要枚举的，这就是剪枝。

于是很容易想到一个策略：首先假设最开始的团是空的，从0节点开始，将其加入候选团，然后遍历剩余所有节点：如果某节点可以与候选团形成团，则加入候选团，然后继续遍历剩余节点；如果不可以（剪枝），就抛弃掉，并回溯到上一个成团的节点。这实际上是按照深度优先的策略对解空间树进行遍历的过程。

按照这个方法，可以求解出所有可能的团，称为平凡团。如果需要求极大团，则对所有的平凡团按照子集关系进行合并，最终留下互不包含的若干个团，即为极大团。取顶点数最多的（若干个）极大团，即为最大团。

## 后记

极大团算法其实是比较困难的算法，难就难在时间复杂度超高，问题规模稍微大一点就不能用。作为NP完全问题，此问题在理论上的意义也很重大。不过暂时没有兴趣进一步研究这个问题，浅尝辄止就好。

问题的关键在于，如何把新遇到的问题，转化到已有的问题上去。

## 参考资料

+ [最大团问题和图的m着色问题](https://blog.csdn.net/liufeng_king/article/details/8781554)
+ [The Clique Algorithm](http://www.dharwadker.org/clique/)


------

# 基于FFT的大数乘法算法

2018-09-10

## 动机

大数乘法是很经典的一个问题，说简单也简单，说难也难。从最简单的错位相乘，到加入分治以提高效率，再到各种高效算法，是学习数值计算的很好的切入点。

大数乘法算法可以突破数据宽度的限制，保持运算结果的位数，因此在金融等场景中非常有用。在实现MikuRec解释器的时候，测试阶乘会遇到非常大的数字相乘，为了方便调试，打算使用大数相乘算法，取代JS语言提供的乘法。长远来看，这也有利于实现CAS之类的系统。

本文只说明并演示基于FFT的大数乘法算法。

## 原理和实现

两个大整数相乘，本质上是卷积操作。卷积的时间复杂度是O(n<sup>2</sup>)。在数字位数很大的时候，效率不是很理想。

为了提高效率，可以利用卷积定理，通过快速傅里叶变换，将时间复杂度压缩到O(n*log(n))的级别。卷积定理指的是，两个序列卷积的傅里叶变换，等于两个序列傅里叶变换的按位相乘。因此，两个大数相乘，可以按照如下算法进行：

+ 对两个大数分别做FFT，得到两个新序列。
+ 两个序列按位相乘，得到傅里叶变换的乘积序列。这一步的时间复杂度是线性的。
+ 对乘积序列作傅里叶反变换，即为大数相乘结果。

```:javascript
// 2018.10.10 大数乘法
function bigIntMultiply(astr, bstr) {
    // 自适应FFT长度
    let maxlen = Math.max(astr.length, bstr.length) * 2;
    let loglen = Math.round(Math.log2(maxlen));
    let FFTSIZE = (loglen % 2 === 0) ? POW[loglen+2] : POW[loglen+1]; // 偶数POW
    // 字符串->复数序列
    let a = new Array(FFTSIZE);
    let b = new Array(FFTSIZE);
    for(let i = 0; i < FFTSIZE; i++) {
        aDigit = (i < astr.length) ? parseInt(astr[astr.length-1-i]) : 0;
        bDigit = (i < bstr.length) ? parseInt(bstr[bstr.length-1-i]) : 0;
        a[i] = new Complex(aDigit, 0);
        b[i] = new Complex(bDigit, 0);
    }
    // 傅氏变换
    let A = FFT(a, FFTSIZE);
    let B = FFT(b, FFTSIZE);
    // 卷积
    let C = new Array();
    for(let i = 0; i < FFTSIZE; i++) {
        let c = A[i].mul(B[i]);
        C.push(c);
    }
    let c = IFFT(C, FFTSIZE);
    // 以字符串输出
    return (function(n) {
        let numstr = '';
        let carry = 0;
        for(let i = 0; i < n.length; i++) {
            let c = Math.round(n[i].rep) + carry;
            if(c >= 0 && c <= 9) {
                numstr = c.toString() + numstr;
                carry = 0;
            }
            else {
                numstr = (c % 10).toString() + numstr;
                carry = Math.round((c - c % 10) / 10);
            }
        }
        return numstr.replace(/^0*/gi, '');
    })(c);
}
```

FFT涉及浮点数运算，存在精度问题。如果使用不涉及浮点数运算的快速数论变换（NTT），就可以避免这个问题。

## 参考资料

+ [Schönhage–Strassen algorithm](https://en.wikipedia.org/wiki/Sch%C3%B6nhage%E2%80%93Strassen_algorithm)
+ [大数乘法（快速傅立叶变换）](https://blog.csdn.net/u013351484/article/details/48739415)
+ [FFT详解&大数乘法](https://blog.csdn.net/ripped/article/details/70241716)

------

# 对数时间码位倒置

2016-09-08

在FFT算法中，有一步是对数组下标进行“码位倒置”，具体说来就是把一个数字的各二进制位倒过来写。

最直接的想法是使用移位操作将输入数字的各个二进制位提取出来，然后按照相反的顺序压入另一个数字。但是，如果设下标二进制位数为n，那么这种算法的时间复杂度是$O(n)$。

下面是一种更快、更适合硬件的码位倒置算法。对于八位无符号整数，只需要三步操作即可。

```:C
#include <stdio.h>

unsigned char bit_reverse(unsigned char i)
{
    unsigned char n = i;
    n = ( (n&0x55)<<1 ) | ( (n&0xaa)>>1 );
    n = ( (n&0x33)<<2 ) | ( (n&0xcc)>>2 );
    n = ( (n&0x0f)<<4 ) | ( (n&0xf0)>>4 );
    return n;
}

int main(void)
{
    unsigned char n = 0;

    while(1)
    {
        printf("Please input the number = ");
        scanf("%d", &n);
        n = bit_reverse(n);
        printf("The reversed number is  = %d\n", n);
    }

    return 0;
}
```

该算法可以将码位倒置算法的时间复杂度控制在$O(\mathrm {log}(n))$，挺机智的。

这个算法是在编写FFT算法的时候找到的。



#!style

.edit_distance_charblock {
    display: inline-block;
    font-size: 15px;
    height: 22px;
    line-height: 22px;
    padding: 2px 5px 2px 5px;
    margin: 0 2px 0 2px;
    background-color: rgb(190,229,245);
    color: #233333;
}
.edit_distance_highlight {
    background-color: pink;
}
.edit_distance_line {
    padding: 2px;
}
.edit_distance_table {
    font-size: 14px;
    border: 1px solid #dddddd;
    border-collapse: collapse;
    background-color: transparent;
    border-spacing: 0;
    font-weight: normal;
}
.edit_distance_table td {
    text-align: center;
    vertical-align: middle;
    border: 1px solid #dddddd;
    padding: 0px 8px 0px 8px;
    /* min-width: 25px; */
}
.edit_distance_table tr:nth-child(1) td, .edit_distance_table td:nth-child(1) {
    background-color: #f2f2f2;
    font-weight: bold;
}

#!script

#!script:./ts/levenshtein.js

#!script:./ts/nagao.js
#!script:./ts/dict.js

#!script:./ts/clique.js

$(function(){

    //////////////////////////////////////////////////
    // 编辑距离
    //////////////////////////////////////////////////
    let editDistance = null;
    let initFlag = false;
    document.getElementById('auto').addEventListener('click', ()=> {
        let argv = new Array();
        argv[0] = document.getElementById('input1').value;
        argv[1] = document.getElementById('input2').value;
        let editDistance = new LevenshteinDistance(argv[0], argv[1]);
        editDistance.auto();
    });
    
    document.getElementById('step').addEventListener('click', ()=> {
        let argv = new Array();
        argv[0] = document.getElementById('input1').value;
        argv[1] = document.getElementById('input2').value;
        if(!initFlag) {
            initFlag = true;
            editDistance = new LevenshteinDistance(argv[0], argv[1]);
        }
        editDistance.step();
    });



    //////////////////////////////////////////////////
    // Nagao
    //////////////////////////////////////////////////
    $('#nagao').click(function() {
        let nw = newword(text);
        // 过滤掉已经在词典的、凝固度小于100的、左右熵小于1.0的词
        let nnw = new Object();
        for(let t in nw) {
            if(!(dict.indexOf(t) >= 0) && t.length >= 2) {
                // if(nw[t]['ent'] >= 1.0 && nw[t]['solidity'] >= 100) {
                    nnw[t] = nw[t];
                // }
            }
        }
        console.log(nnw);
    });


    //////////////////////////////////////////////////
    // 极大团
    //////////////////////////////////////////////////
    const a = [
    //   0  1  2  3  4  5  6  7  8  9
        [0, 1, 1, 1, 0, 1, 1, 0, 1, 0],
        [1, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 0, 1, 1, 0, 0, 0, 0, 0],
        [1, 1, 1, 0, 1, 0, 1, 1, 0, 0],
        [0, 0, 1, 1, 0, 1, 0, 0, 0, 0],
        [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],
        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    ];
    let g = new Graph(10, a);
    $('#maximal_cliques').html(JSON.stringify(g.calculateMaximalCliques()));






});
